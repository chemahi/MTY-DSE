Introducción a la minería de texto con R
========================================================
author: Carl W. Handlin
date: 4-Feb-2016
font-import: http://fonts.googleapis.com/css?family=Montserrat
font-family: 'Montserrat'
css: styles.css
width: 1280
height: 800

R
========================================================
- Lenguaje interpretado y dialecto del lenguaje "S"
- Software libre con enfoque estadístico y de análisis de datos.
- Poderoso y gratuito IDE "RStudio"
- Se puede ampliar a través de paquetes vía CRAN (~4,000)

https://cran.r-project.org/

Minería de texto
========================================================

"Text mining is the process of combing through countless pages of plain-language digitized text to find useful information that’s been hiding in plain sight." [1]

### ¿Por qué usar minería de texto?

- Se estima que entre el 70%-80% de la información se encuentra en forma de datos no estructurados [2]. (Para las computadoras)

Minería de texto
========================================================

En un minuto se crean [3]:

- 204 Millones de emails
- 2.46 Millones de posts en Facebook
- 320,000 Tweets
- 54,000 posts en Tumblr
- 17 artículos de Wikipedia

Además de una infinidad de: Reportes, artículos en internet, comentarios, customer reviews, etc.

Text Mining vs NLP
========================================================

- El objetivo de la minería de texto es el descubrimiento y la extracción de conocimiento, no trivial, de texto no estructurado. (Orientado al análisis de datos)

- Natural Language Processing (NLP) hace el intento de extraer información más significativa haciendo uso de las reglas gramáticas y de la lingüística. (Orientado a la inteligencia artificial)

Aplicaciones
========================================================

- Filtros de SPAM
- Sistemas de recomendación
- Detección de fraude
- Monitoreo de la opinión pública
- Inteligencia de negocios
- Análisis de documentos legales y administrativos

Ejemplo: What's Cooking?
========================================================

- Competencia de Kaggle en 2015
- objetivo: categorizar recetas de cocina usando sus ingredientes.
- dataset: http://www.yummly.com/

<img src="http://www.3scale.net/wp-content/uploads/2012/03/customer-yummly-API.png" style="background-color:transparent; border:0px; box-shadow:none;"></img>

Cargar los datos
========================================================

```{r, cache=TRUE}
library(jsonlite)
train <- fromJSON("train.json", flatten = TRUE)[-1]
head(train, 1)
```

Explorar los datos
========================================================

```{r, echo=FALSE, fig.width=8,fig.height=4.5,dpi=300,out.width="1280px",height="800px"}
library(ggplot2)
ggplot(data = train, aes(x = cuisine)) + 
    geom_bar() +
    labs(title = "Numero de recetas", x = "Tipo de cocina", y = "Numero de recetas") +
    theme(axis.text.x  = element_text(angle=90, vjust=0.5, size=16))
```

R package 'tm'
========================================================

tm: Text Mining.  
Autores: Ingo Feinerer, Kurt Hornik.

'tm' es un framework que facilita la minería de texto usando R.

```{r}
library(tm)
```

Nos permite: pre procesar, asociar, agrupar,  resumir y categorizar texto.

R package 'caret'
========================================================

caret: Classification and Regression Training.  
Autor: Max Kuhn.

'caret' es un conjunto de funciones en R para agilizar la creación de modelos predictivos.

```{r}
library(caret)
```

repo: http://topepo.github.io/caret/index.html

Corpus
========================================================

En la minería de texto, la clase fundamental para la manipulación de datos se denomina "Corpus".

Corpus es solamente una colección de textos a los cuales se les desea extraer características.

```{r, cache=TRUE}
ingredients <- Corpus(VectorSource(train$ingredients))
ingredients
```

Pre procesamiento
========================================================

La librería 'tm' nos permite manipular los textos.

```{r}
getTransformations()
```

- removeNumbers(): nos permite quitar los números en los textos
- removePunctuation(): nos permite remover los signos de puntuación
- removeWords(): nos permite remover ciertas palabras tales como "stopwords"
- stripWhitespace(): elimina espacios en blanco extras de un documento de texto
- stemDocument(): ?

Stemming
========================================================

- Es un método para reducir una palabra a su raíz (stem)

Ejemplo:
"stems", "stemmer", "stemming", "stemmed" tienen como base: "stem"

- La libreria 'tm' nos deja usar el algoritmo de Porter

http://tartarus.org/martin/PorterStemmer/

Pre procesamiento
========================================================

Pre procesamos nuestro Corpus de recetas para continuar con el análisis:

```{r, cache=TRUE}
library(SnowballC)
ingredients <- tm_map(ingredients, removeNumbers)
ingredients <- tm_map(ingredients, removePunctuation)
ingredients <- tm_map(ingredients, stemDocument)
ingredients <- tm_map(ingredients, stripWhitespace)
```

Document Term Matrix (DTM)
========================================================

Es una matriz matemática que describe la frecuencia con la que aparecen los términos en una colección de documentos

```{r, cache=TRUE}
ingredientsDTM <- DocumentTermMatrix(ingredients)
inspect(ingredientsDTM[1:5,1:5])
```

Wordclouds
========================================================

- Son una representación visual de las palabras que conforman un texto, en donde el tamaño es mayor para las palabras que aparecen con más frecuencia.

- Se pueden usar colores para denotar otras características

```{r}
library(wordcloud)
```

Wordclouds
========================================================

```{r, echo=FALSE, fig.width=8,fig.height=4.5,dpi=300,out.width="1280px",height="800px"}
m <- as.matrix(ingredientsDTM)
v <- sort(colSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
set.seed(1989)
wordcloud(words = d$word,scale=c(2,.5), freq = d$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(5, "Dark2"))
```

Frecuencia de términos
========================================================

El default de DTM es tf, que se refiere a solamente la frecuencia con la que aparecen los términos.

Usamos ahorita tf-idf (Term frequency – Inverse document frequency) nos ayuda a reflejar la importancia de una palabra en un documento.

```{r, echo=FALSE, cache=TRUE}
ingredientsDTM <- DocumentTermMatrix(ingredients, control = list(weighting = function(x) weightTfIdf(x,normalize = FALSE)))
ingredientsDTM
```

Wordclouds
========================================================

```{r, echo=FALSE, fig.width=8,fig.height=4.5,dpi=300,out.width="1280px",height="800px"}
m <- as.matrix(ingredientsDTM)
v <- sort(colSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
set.seed(1989)
wordcloud(words = d$word,scale=c(2,.5), freq = d$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(5, "Dark2"))
```

Remove Sparse Terms (Feature selection)
========================================================

Nuestro DTM contiene una gran cantidad de columnas (ingredientes).

La reducción del número de columnas mediante la eliminación de ingredientes que no ocurren con frecuencia, puede ayudar con el modelo (aunque a veces únicos ingredientes pueden ser clave para predecir el tipo de comida).

```{r, remove_sparse_terms, echo = TRUE, cache=TRUE}
sparse <- removeSparseTerms(ingredientsDTM, 0.999)
ingredientsDTM <- as.data.frame(as.matrix(sparse))
ingredientsDTM$cuisine <- as.factor(train$cuisine)
```

Entrenamiento del modelo
========================================================

Ahora estamos listos para hacer nuestro clasificador, hacemos la partición de 80/20

```{r, cache=TRUE}
inTrain <- createDataPartition(y = ingredientsDTM$cuisine, p = 0.8, list = FALSE)
training <- ingredientsDTM[inTrain,]
testing <- ingredientsDTM[-inTrain,]
```

Entrenamiento del modelo
========================================================

Hacemos un modelo sencillo usando SVM y Cross-validation

```{r, echo=FALSE}
set.seed(1989)
svm_model <-train(cuisine ~ ., data=training[1:1000,], method = "svmLinear",
                 trControl = trainControl(method = "cv"))
```

```{r, cache=TRUE}
print(svm_model)
```

Evaluación del modelo
========================================================

```{r, cache=TRUE}
svm_predict <- predict(svm_model, newdata = testing)
svm_cm <- confusionMatrix(svm_predict, testing$cuisine)
svm_cm
```

Repo
========================================================

https://github.com/cwallaceh/MTY-DSE

Referencias
========================================================

[1] Belsky, G. (2012, March 20). Why Text Mining May Be The Next Big Thing. Retrieved February 03, 2016, from http://business.time.com/2012/03/20/why-text-mining-may-be-the-next-big-thing/  
[2] Holzinger, A. (2013). Human-computer interaction and knowledge discovery in complex, unstructured, big data: Third international workshop, HCI - KDD 2013, held at SouthCHI 2013, Maribor, Slovenia, July 1-3, 2013 ; proceedings. Berlin: Springer.  
[3] Redmore, S. (2014, March 18). How text mining can help your business dig gold. Retrieved February 03, 2016, from http://www.techradar.com/news/internet/web/how-text-mining-can-help-your-business-dig-gold-1229440  
-Ingo Feinerer, Kurt Hornik, David Meyer. Text Mining Infrastructure in R, Vol. 25, Issue 5, Mar 2008, Journal of Statistical Software.  
-Kao, Anne, and Stephen R. Poteet. Natural Language Processing and Text Mining. London: Springer, 2007. Web.  

